{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b20a6068",
      "metadata": {
        "id": "b20a6068"
      },
      "source": [
        " ## Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41663703",
      "metadata": {
        "id": "41663703"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "276cff3c",
      "metadata": {
        "id": "276cff3c"
      },
      "source": [
        "## Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "896b2626",
      "metadata": {
        "id": "896b2626"
      },
      "outputs": [],
      "source": [
        "model = models.vgg19(pretrained=True).features\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e466e1",
      "metadata": {
        "id": "48e466e1"
      },
      "outputs": [],
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        self.chosen_features = ['0', '5', '10', '19', '28']\n",
        "        self.model = models.vgg19(pretrained=True).features[:29]\n",
        "    \n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for layer_num, layer in enumerate(self.model):\n",
        "            x = layer(x)\n",
        "            if str(layer_num) in self.chosen_features:\n",
        "                features.append(x)\n",
        "        return features "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8254528",
      "metadata": {
        "id": "a8254528"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cpu')\n",
        "\n",
        "image_size = 356\n",
        "\n",
        "loader = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def load_image(image_name):\n",
        "    image = Image.open(image_name)\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b78ee9f",
      "metadata": {
        "id": "5b78ee9f"
      },
      "outputs": [],
      "source": [
        "original_img = load_image('input.jpg')\n",
        "style_img = load_image('style.jpg')\n",
        "generated_img = original_img.clone().requires_grad_(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dd11321",
      "metadata": {
        "id": "4dd11321"
      },
      "outputs": [],
      "source": [
        "steps = 6000\n",
        "learning_rate = 0.001\n",
        "alpha = 1\n",
        "beta = 0.01\n",
        "optimizer = optim.Adam([generated_img], lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_img.shape"
      ],
      "metadata": {
        "id": "9q3deetVkJPF"
      },
      "id": "9q3deetVkJPF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_features = model(generated_img)\n",
        "generated_features.shape"
      ],
      "metadata": {
        "id": "7FvpuyVHjttd"
      },
      "id": "7FvpuyVHjttd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "672b8126",
      "metadata": {
        "id": "672b8126"
      },
      "outputs": [],
      "source": [
        "model = VGG().to(device).eval()\n",
        "for step in range(steps):\n",
        "    generated_features = model(generated_img)\n",
        "    original_img_features = model(original_img)\n",
        "    style_features = model(style_img)\n",
        "    \n",
        "    style_loss = original_loss = 0\n",
        "    for gen_feature, orig_feature, style_feature in zip(generated_features, original_img_features, style_features):\n",
        "            batch_size, channel, height, width = gen_feature.shape\n",
        "            original_loss += torch.mean((gen_feature - orig_feature)**2)\n",
        "            \n",
        "            G = gen_feature.view(channel, height*width).mm(\n",
        "            gen_feature.view(channel, height*width).t())\n",
        "            S = style_feature.view(channel, height*width).mm(\n",
        "            style_feature.view(channel, height*width).t())\n",
        "            \n",
        "            style_loss += torch .mean((G-S)**2)\n",
        "    total_loss = alpha*original_loss + beta*style_loss\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "        \n",
        "    if step % 200 == 0:\n",
        "        print(f'step {step} : {total_loss}')\n",
        "        save_image (generated_img, 'generated.png')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}